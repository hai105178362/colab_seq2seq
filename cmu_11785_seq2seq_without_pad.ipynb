{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cmu 11785 seq2seq without pad.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1iuKRKHeL-wTvTaVl6t6TUyXsRjoiKcIa",
      "authorship_tag": "ABX9TyMIv2TfsSwo2rnTPvTQhwfJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hai105178362/colab_seq2seq/blob/master/cmu_11785_seq2seq_without_pad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxZHFFNWF1Vt",
        "colab_type": "code",
        "outputId": "67385340-ef12-476b-ed7c-96093176ddfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(\"hello\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOcXdOwaGhYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjXxw_K5aCs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import *\n",
        "\n",
        "  \n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils as utils\n",
        "\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import time\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyukIq10aGHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Loading all the numpy files containing the utterance information and text information\n",
        "'''\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    speech_train = np.load('/content/drive/My Drive/Colab Notebooks/Data Sets/11 785/hw4/train_new.npy', allow_pickle=True, encoding='bytes')\n",
        "    speech_valid = np.load('/content/drive/My Drive/Colab Notebooks/Data Sets/11 785/hw4/dev_new.npy', allow_pickle=True, encoding='bytes')\n",
        "    speech_test = np.load('/content/drive/My Drive/Colab Notebooks/Data Sets/11 785/hw4/test_new.npy', allow_pickle=True, encoding='bytes')\n",
        "\n",
        "    transcript_train = np.load('/content/drive/My Drive/Colab Notebooks/Data Sets/11 785/hw4/train_transcripts.npy', allow_pickle=True,encoding='bytes')\n",
        "    transcript_valid = np.load('/content/drive/My Drive/Colab Notebooks/Data Sets/11 785/hw4/dev_transcripts.npy', allow_pickle=True, encoding='bytes')\n",
        "\n",
        "    return speech_train, speech_valid, speech_test, transcript_train, transcript_valid\n",
        "\n",
        "\n",
        "'''\n",
        "Transforms alphabetical input to numerical input, replace each letter by its corresponding \n",
        "index from letter2index\n",
        "'''\n",
        "\n",
        "\n",
        "def transform_letter_to_index(transcript, letter2index):\n",
        "    \"\"\"\n",
        "    :param transcript :(N, ) Transcripts are the text input\n",
        "    :param letter2index: letter2index dict\n",
        "    :return letter_to_index_list: Returns a list for all the transcript sentence to index\n",
        "    \"\"\"\n",
        "    full_res = []\n",
        "\n",
        "    for cur_sentence in transcript:\n",
        "        cur_res = [letter2index[\"<sos>\"], ]\n",
        "        \n",
        "        for cur_word in cur_sentence:\n",
        "            cur_res += [letter2index[c] for c in cur_word.decode(\"utf-8\")]\n",
        "            cur_res.append(letter2index[\" \"])\n",
        "\n",
        "        # pop the last space\n",
        "        cur_res.pop()\n",
        "        cur_res.append(letter2index[\"<eos>\"])\n",
        "\n",
        "        full_res.append(np.array(cur_res))\n",
        "            \n",
        "    return np.array(full_res)\n",
        "\n",
        "\n",
        "def transform_index_to_letter(index_arr, letter_list):\n",
        "    \"\"\"\n",
        "    :param index_arr :(N, ) index\n",
        "    :param letter_list: index2index dict\n",
        "    :return transcript:\n",
        "    \"\"\"\n",
        "    transcript = \"\".join([letter_list[i] for i in index_arr[1:-1]])\n",
        "\n",
        "    return transcript\n",
        "\n",
        "\n",
        "'''\n",
        "Optional, create dictionaries for letter2index and index2letter transformations\n",
        "'''\n",
        "\n",
        "\n",
        "def create_dictionaries(letter_list):\n",
        "    letter2index = {y: x for x, y in enumerate(letter_list)}\n",
        "    index2letter = {x: y for x, y in enumerate(letter_list)}\n",
        "    return letter2index, index2letter\n",
        "\n",
        "\n",
        "class Speech2TextDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class for the speech to text data, this may need some tweaking in the\n",
        "    getitem method as your implementation in the collate function may be different from\n",
        "    ours.\n",
        "    \"\"\"\n",
        "    def __init__(self, speech, text=None, is_train=True):\n",
        "        self.speech = speech\n",
        "        self.speech_len = [len(x) for x in speech]\n",
        "\n",
        "        self.is_train = is_train\n",
        "        if text is not None:\n",
        "            self.text = text\n",
        "            self.text_len = [len(x) for x in text]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.speech.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.is_train:\n",
        "            return torch.tensor(self.speech[index].astype(np.float32)), self.speech_len[index], \\\n",
        "                   torch.tensor(self.text[index]), self.text_len[index] - 1,\n",
        "        else:\n",
        "            return torch.tensor(self.speech[index].astype(np.float32)), self.speech_len[index]\n",
        "\n",
        "\n",
        "def collate_train(batch_data):\n",
        "    # Return the padded speech and text data, and the length of utterance and transcript ###\n",
        "    cur_speech, cur_speech_len, cur_text, cur_text_len = zip(*batch_data)\n",
        "    cur_speech = pad_sequence(cur_speech)\n",
        "    cur_text = pad_sequence(cur_text, batch_first=True)\n",
        "\n",
        "    return cur_speech, torch.tensor(cur_speech_len, dtype=torch.int64), \\\n",
        "           cur_text, torch.tensor(cur_text_len, dtype=torch.int64)\n",
        "\n",
        "\n",
        "def collate_test(batch_data):\n",
        "    # Return padded speech and length of utterance ###\n",
        "    cur_speech, cur_speech_len = zip(*batch_data)\n",
        "    cur_speech = pad_sequence(cur_speech)\n",
        "\n",
        "    return cur_speech, torch.tensor(cur_speech_len, dtype=torch.int64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH3W3w5naXv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "SOS_INDEX = 33\n",
        "EOS_INDEX = 34\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention is calculated using key, value and query from Encoder and decoder.\n",
        "    Below are the set of operations you need to perform for computing attention:\n",
        "        energy = bmm(key, query)\n",
        "        attention = softmax(energy)\n",
        "        context = bmm(attention, value)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "    def forward(self, query, key, value, lens):\n",
        "        \"\"\"\n",
        "        :param query :(N, context_size) Query is the output of LSTMCell from Decoder\n",
        "        :param key: (N, key_size) Key Projection from Encoder per time step\n",
        "        :param value: (N, value_size) Value Projection from Encoder per time step\n",
        "        :param lens: (N)\n",
        "        :return output: Attended Context\n",
        "        :return attention_mask: Attention mask that can be plotted\n",
        "        \"\"\"\n",
        "        key = torch.transpose(key, 0, 1)\n",
        "        value = torch.transpose(value, 0, 1)\n",
        "        query = query.unsqueeze(2)\n",
        "\n",
        "        mask = (torch.arange(value.shape[1]).reshape((-1, 1)) >= lens).transpose(0, 1).to(DEVICE)\n",
        "\n",
        "        energy = torch.bmm(key, query).squeeze(2)\n",
        "        energy.masked_fill_(mask, -1e9)\n",
        "        energy = energy.unsqueeze(2)\n",
        "\n",
        "        attention = torch.softmax(energy, dim=1)\n",
        "\n",
        "        attention = torch.transpose(attention, 1, 2)\n",
        "\n",
        "        context = torch.bmm(attention, value).squeeze(1)\n",
        "\n",
        "        return context, mask\n",
        "\n",
        "\n",
        "class pBLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Pyramidal BiLSTM\n",
        "    The length of utterance (speech input) can be hundereds to thousands of frames long.\n",
        "    The Paper reports that a direct LSTM implementation as Encoder resulted in slow convergence,\n",
        "    and inferior results even after extensive training.\n",
        "    The major reason is inability of AttendAndSpell operation to extract relevant information\n",
        "    from a large number of input steps.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(pBLSTM, self).__init__()\n",
        "        self.lstm_1 = nn.LSTM(input_size=input_dim * 2, hidden_size=hidden_dim, num_layers=1, bidirectional=True)\n",
        "        self.lstm_2 = nn.LSTM(input_size=hidden_dim * 4, hidden_size=hidden_dim, num_layers=1, bidirectional=True)\n",
        "        self.lstm_3 = nn.LSTM(input_size=hidden_dim * 4, hidden_size=hidden_dim, num_layers=1, bidirectional=True)\n",
        "\n",
        "        self.lstm_layers = nn.ModuleList([self.lstm_1, self.lstm_2, self.lstm_3])\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x :(T, N, H) input to the pBLSTM\n",
        "        :return output: (N, T, H) encoded sequence from pyramidal Bi-LSTM\n",
        "        \"\"\"\n",
        "\n",
        "        x, lens = utils.rnn.pad_packed_sequence(x, batch_first=False)\n",
        "\n",
        "        x = torch.transpose(x, 0, 1)\n",
        "        x = x[:x.shape[0], :x.shape[1] // 2 * 2, :]\n",
        "        x = x.reshape((x.shape[0], x.shape[1] // 2, x.shape[2] * 2))\n",
        "        x = torch.transpose(x, 0, 1)\n",
        "        lens = lens // 2\n",
        "\n",
        "        cur_inp = utils.rnn.pack_padded_sequence(x, lengths=lens, batch_first=False, enforce_sorted=False)\n",
        "        x, _ = self.lstm_1(cur_inp)\n",
        "\n",
        "        x, lens = utils.rnn.pad_packed_sequence(x, batch_first=False)\n",
        "\n",
        "        x = torch.transpose(x, 0, 1)\n",
        "        x = x[:x.shape[0], :x.shape[1] // 2 * 2, :]\n",
        "        x = x.reshape((x.shape[0], x.shape[1] // 2, x.shape[2] * 2))\n",
        "        x = torch.transpose(x, 0, 1)\n",
        "        lens = lens // 2\n",
        "\n",
        "        cur_inp = utils.rnn.pack_padded_sequence(x, lengths=lens, batch_first=False, enforce_sorted=False)\n",
        "        x, _ = self.lstm_2(cur_inp)\n",
        "\n",
        "        x, lens = utils.rnn.pad_packed_sequence(x, batch_first=False)\n",
        "\n",
        "        x = torch.transpose(x, 0, 1)\n",
        "        x = x[:x.shape[0], :x.shape[1] // 2 * 2, :]\n",
        "        x = x.reshape((x.shape[0], x.shape[1] // 2, x.shape[2] * 2))\n",
        "        x = torch.transpose(x, 0, 1)\n",
        "        lens = lens // 2\n",
        "\n",
        "        cur_inp = utils.rnn.pack_padded_sequence(x, lengths=lens, batch_first=False, enforce_sorted=False)\n",
        "        x, _ = self.lstm_3(cur_inp)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder takes the utterances as inputs and returns the key and value.\n",
        "    Key and value are nothing but simple projections of the output from pBLSTM network.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, value_size, key_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.base_lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bidirectional=True)\n",
        "        self.pblstm = pBLSTM(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "        self.key_network = nn.Linear(hidden_dim * 2, key_size)\n",
        "        self.value_network = nn.Linear(hidden_dim * 2, value_size)\n",
        "\n",
        "    def forward(self, x, lens):\n",
        "        rnn_inp = utils.rnn.pack_padded_sequence(x, lengths=lens, batch_first=False, enforce_sorted=False)\n",
        "\n",
        "        packed_out, _ = self.base_lstm(rnn_inp)\n",
        "\n",
        "        # outputs (max_seq_len / 4, batch_size, hidden_size * 2)\n",
        "        packed_out = self.pblstm(packed_out)\n",
        "\n",
        "        # outputs (max_seq_len / 4, batch_size, hidden_size * 2)\n",
        "        # len_out (batch_size)\n",
        "        # For tests: outputs (130, 64, 256), len_out [55, 58, 66, 60, 62, ...]\n",
        "        linear_input, lens_out = utils.rnn.pad_packed_sequence(packed_out)\n",
        "\n",
        "        keys = self.key_network(linear_input)\n",
        "        value = self.value_network(linear_input)\n",
        "\n",
        "        return keys, value, lens_out\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    As mentioned in a previous recitation, each forward call of decoder deals with just one time step,\n",
        "    thus we use LSTMCell instead of LSLTM here.\n",
        "    The output from the second LSTMCell can be used as query here for attention module.\n",
        "    In place of value that we get from the attention, this can be replace by context we get from the attention.\n",
        "    Methods like Gumble noise and teacher forcing can also be incorporated for improving the performance.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, hidden_dim, value_size, key_size, is_attended):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_dim, padding_idx=0)\n",
        "        self.lstm1 = nn.LSTMCell(input_size=hidden_dim + value_size, hidden_size=hidden_dim)\n",
        "        self.lstm2 = nn.LSTMCell(input_size=hidden_dim, hidden_size=key_size)\n",
        "\n",
        "        self.is_attended = is_attended\n",
        "        if is_attended:\n",
        "            self.attention = Attention()\n",
        "\n",
        "        self.teacher_forcing_rate = 0.6\n",
        "\n",
        "        self.character_prob = nn.Linear(key_size + value_size, vocab_size)\n",
        "\n",
        "    def forward(self, key, values, lens, text=None, is_train=True):\n",
        "        \"\"\"\n",
        "        :param key :(T, N, key_size) Output of the Encoder Key projection layer\n",
        "        :param values: (T, N, value_size) Output of the Encoder Value projection layer\n",
        "        :param lens: (N) lens for key and values\n",
        "        :param text: (N, text_len) Batch input of text with text_length\n",
        "        :param is_train: Train or eval mode\n",
        "        :return predictions: Returns the character prediction probability\n",
        "        \"\"\"\n",
        "        batch_size = key.shape[1]\n",
        "        embeddings = None\n",
        "\n",
        "        if is_train:\n",
        "            max_len = text.shape[1]\n",
        "            # embeddings (batch_size, text_len, hidden_size)\n",
        "            embeddings = self.embedding(text)\n",
        "        else:\n",
        "            max_len = 250\n",
        "\n",
        "        predictions = []\n",
        "        hidden_states = [None, None]\n",
        "        prediction = (torch.ones(batch_size, 1) * SOS_INDEX).to(DEVICE)\n",
        "\n",
        "        attention_score = values.mean(dim=0)\n",
        "\n",
        "        for i in range(max_len):\n",
        "            # * Implement Gumble noise and teacher forcing techniques \n",
        "            # * When attention is True, replace values[i,:,:] with the context you get from attention.\n",
        "            # * If you haven't implemented attention yet, then you may want to check the index and break \n",
        "            #   out of the loop so you do you do not get index out of range errors. \n",
        "\n",
        "            if is_train:\n",
        "\n",
        "                rnd = np.random.rand()\n",
        "\n",
        "                if rnd >= self.teacher_forcing_rate:\n",
        "                    char_embed = embeddings[:, i, :]\n",
        "                else:\n",
        "                    char_embed = self.embedding(prediction.argmax(dim=-1))\n",
        "            else:\n",
        "                char_embed = self.embedding(prediction.argmax(dim=-1))\n",
        "\n",
        "            inp = torch.cat([char_embed, attention_score], dim=1)\n",
        "            hidden_states[0] = self.lstm1(inp, hidden_states[0])\n",
        "\n",
        "            inp_2 = hidden_states[0][0]\n",
        "            hidden_states[1] = self.lstm2(inp_2, hidden_states[1])\n",
        "\n",
        "            # query\n",
        "            output = hidden_states[1][0]\n",
        "\n",
        "            if self.is_attended:\n",
        "                attention_score, attention_mask = self.attention(output, key, values, lens)\n",
        "\n",
        "            prediction = self.character_prob(torch.cat([output, attention_score], dim=1))\n",
        "            predictions.append(prediction.unsqueeze(1))\n",
        "\n",
        "        return torch.cat(predictions, dim=1)\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "    We train an end-to-end sequence to sequence model comprising of Encoder and Decoder.\n",
        "    This is simply a wrapper \"model\" for your encoder and decoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, vocab_size, hidden_dim, value_size, key_size, is_attended=True):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = Encoder(input_dim, hidden_dim, value_size, key_size)\n",
        "        self.decoder = Decoder(vocab_size, hidden_dim, value_size, key_size, is_attended)\n",
        "\n",
        "    def forward(self, speech_input, speech_len, text_input=None, is_train=True):\n",
        "        key, value, lens = self.encoder(speech_input, speech_len)\n",
        "\n",
        "        if is_train:\n",
        "            predictions = self.decoder(key, value, lens, text_input)\n",
        "        else:\n",
        "            predictions = self.decoder(key, value, lens, text=None, is_train=False)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "\n",
        "def greedy_search_gen(outputs):\n",
        "    decoded_outputs = torch.argmax(outputs, dim=2)\n",
        "\n",
        "    decoded_outputs = torch.cat([torch.ones((decoded_outputs.shape[0], 1), dtype=torch.int64).to(DEVICE) * SOS_INDEX,\n",
        "                                 decoded_outputs], dim=1)\n",
        "\n",
        "    cur_text_len = torch.zeros(decoded_outputs.shape[0], dtype=torch.int64).to(DEVICE)\n",
        "    cur_text = []\n",
        "\n",
        "    for i in range(decoded_outputs.shape[0]):\n",
        "        cur_text_len[i] = next(j for j in range(decoded_outputs.shape[1])\n",
        "                               if (decoded_outputs[i][j] == EOS_INDEX or j == decoded_outputs.shape[1] - 1)) + 1\n",
        "\n",
        "        cur_text.append(decoded_outputs[i][:cur_text_len[i]])\n",
        "\n",
        "    return cur_text, cur_text_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45Y28Vffaj4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_loader, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    criterion = criterion.to(DEVICE)\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    pp_loss = None\n",
        "\n",
        "    print(\"train start\")\n",
        "\n",
        "    # 1) Iterate through your loader\n",
        "    for cur_speech, cur_speech_len, cur_text, cur_text_len in train_loader:\n",
        "\n",
        "        # 2) Use torch.autograd.set_detect_anomaly(True) to get notices about gradient explosion\n",
        "        # 3) Set the inputs to the device.\n",
        "        # 4) Pass your inputs, and length of speech into the model.\n",
        "        # outputs = model(cur_speech, cur_speech_len, cur_text)\n",
        "        # 5) Generate a mask based on the lengths of the text to create a masked loss.\n",
        "        # 5.1) Ensure the mask is on the device and is the correct shape.\n",
        "        # 6) If necessary, reshape your predictions and origianl text input\n",
        "        # 6.1) Use .contiguous() if you need to.\n",
        "        # 7) Use the criterion to get the loss.\n",
        "        # 8) Use the mask to calculate a masked loss.\n",
        "        # 9) Run the backward pass on the masked loss.\n",
        "        # 10) Use torch.nn.utils.clip_grad_norm(model.parameters(), 2)\n",
        "        # 11) Take a step with your optimizer\n",
        "        # 12) Normalize the masked loss\n",
        "        # 13) Optionally print the training loss after every N batches\n",
        "\n",
        "        cur_speech = cur_speech.to(DEVICE)  # (max_seq_len, batch_size, utter_len (or 40))\n",
        "        cur_speech_len = cur_speech_len.to(DEVICE)  # (batch_size)\n",
        "        cur_text = cur_text.to(DEVICE)  # (batch_size, max_seq_len)\n",
        "        cur_text_len = cur_text_len.to(DEVICE)  # (batch_size)\n",
        "\n",
        "        outputs = model(cur_speech, cur_speech_len, cur_text, True)\n",
        "\n",
        "        outputs_mask = torch.arange(cur_text.shape[1]).reshape((-1, 1)).to(DEVICE) < cur_text_len\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = None\n",
        "        n_tokens = cur_text_len.sum()\n",
        "\n",
        "        for i in range(cur_text.size(1) - 1):\n",
        "            cur_output = outputs[:, i, :]\n",
        "            active = outputs_mask[i, :]\n",
        "\n",
        "            if loss is None:\n",
        "                loss = criterion(cur_output[active], cur_text[active, i + 1])\n",
        "            else:\n",
        "                loss += criterion(cur_output[active], cur_text[active, i + 1])\n",
        "\n",
        "        loss /= n_tokens\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pp_loss = torch.exp(loss)\n",
        "\n",
        "    print(\"training loss of {} epoch\".format(epoch))\n",
        "    print(pp_loss)\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "\n",
        "def val(model, val_loader, criterion, epoch):\n",
        "    model.eval()\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    criterion = criterion.to(DEVICE)\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    pp_loss = None\n",
        "\n",
        "    for cur_speech, cur_speech_len, cur_text, cur_text_len in val_loader:\n",
        "\n",
        "        cur_speech = cur_speech.to(DEVICE)  # (max_seq_len, batch_size, utter_len (or 40))\n",
        "        cur_speech_len = cur_speech_len.to(DEVICE)  # (batch_size)\n",
        "        cur_text = cur_text.to(DEVICE)  # (batch_size, max_seq_len)\n",
        "        cur_text_len = cur_text_len.to(DEVICE)  # (batch_size)\n",
        "\n",
        "        outputs = model(cur_speech, cur_speech_len, cur_text, True)\n",
        "\n",
        "        outputs_mask = torch.arange(cur_text.shape[1]).reshape((-1, 1)).to(DEVICE) < cur_text_len\n",
        "\n",
        "        loss = None\n",
        "        n_tokens = cur_text_len.sum()\n",
        "\n",
        "        for i in range(cur_text.size(1) - 1):\n",
        "            cur_output = outputs[:, i, :]\n",
        "            active = outputs_mask[i, :]\n",
        "\n",
        "            if loss is None:\n",
        "                loss = criterion(cur_output[active], cur_text[active, i + 1])\n",
        "            else:\n",
        "                loss += criterion(cur_output[active], cur_text[active, i + 1])\n",
        "\n",
        "        loss /= n_tokens\n",
        "\n",
        "        pp_loss = torch.exp(loss)\n",
        "\n",
        "    print(\"validation loss of {} epoch\".format(epoch))\n",
        "    print(pp_loss)\n",
        "    \n",
        "    end = time.time()\n",
        "    return loss\n",
        "\n",
        "\n",
        "TEST_SEQ_LEN = 200\n",
        "\n",
        "\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    full_text = []\n",
        "    full_text_len = None\n",
        "\n",
        "    for cur_speech, cur_speech_len in test_loader:\n",
        "\n",
        "        cur_speech = cur_speech.to(DEVICE)  # (max_seq_len, batch_size, utter_len (or 40))\n",
        "        cur_speech_len = cur_speech_len.to(DEVICE)  # (batch_size)\n",
        "\n",
        "        outputs = model(cur_speech, cur_speech_len, None, False)\n",
        "\n",
        "        cur_text, _ = greedy_search_gen(outputs)\n",
        "\n",
        "        # if full_text_len is None:\n",
        "        #     full_text_len = cur_text_len\n",
        "        # else:\n",
        "        #     full_text_len = torch.cat([full_text_len, cur_text_len])\n",
        "\n",
        "        full_text += cur_text\n",
        "\n",
        "    return full_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1Xic9iqavH3",
        "colab_type": "code",
        "outputId": "77f9f4b2-904b-4776-c7d0-2f821e5bb945",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "LETTER_LIST = ['<pad>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q',\n",
        "               'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '-', \"'\", '.', '_', '+', ' ', '<sos>', '<eos>']\n",
        "LETTER2INDEX, INDEX2LETTER = create_dictionaries(LETTER_LIST)\n",
        "\n",
        "\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "def main():\n",
        "    best_valid_loss = float('inf')\n",
        "    model = Seq2Seq(input_dim=40, vocab_size=len(LETTER_LIST), hidden_dim=128, value_size=128, key_size=256,\n",
        "                    is_attended=True)\n",
        "\n",
        "    # cur_model_num = 6\n",
        "    # model.load_state_dict(torch.load('model_{}'.format(cur_model_num)))\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "    n_epochs = 50\n",
        "    batch_size = 64 if DEVICE == 'cuda' else 1\n",
        "\n",
        "    speech_train, speech_valid, speech_test, transcript_train, transcript_valid = load_data()\n",
        "    character_text_train = transform_letter_to_index(transcript_train, LETTER2INDEX)\n",
        "    character_text_valid = transform_letter_to_index(transcript_valid, LETTER2INDEX)\n",
        "\n",
        "    train_dataset = Speech2TextDataset(speech_train, character_text_train)\n",
        "    val_dataset = Speech2TextDataset(speech_valid, character_text_valid)\n",
        "    test_dataset = Speech2TextDataset(speech_test, None, False)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_train)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_train)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_test)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        start_time = time.time()\n",
        "        train(model, train_loader, criterion, optimizer, epoch)\n",
        "        valid_loss = val(model, val_loader, criterion, epoch)\n",
        "        if valid_loss < best_valid_loss:\n",
        "          best_valid_loss = valid_loss\n",
        "          torch.save(model.state_dict(), '11785-seq2seq-model.pt')\n",
        "        end_time = time.time()\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {torch.exp(valid_loss):7.3f}')\n",
        "    # test(model, test_loader)\n",
        "\n",
        "\n",
        "    result_gen(test_loader, 0)\n",
        "    print(\"finish\")\n",
        "\n",
        "\n",
        "def result_gen(test_loader, model_num):\n",
        "    model = Seq2Seq(input_dim=40, vocab_size=len(LETTER_LIST), hidden_dim=128, value_size=128, key_size=256,\n",
        "                    is_attended=True)\n",
        "\n",
        "    model.load_state_dict(torch.load('11785-seq2seq-model.pt'))\n",
        "    model.eval()\n",
        "\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    test_text = test(model, test_loader)\n",
        "\n",
        "    test_text_str = []\n",
        "\n",
        "    for cur_text in test_text:\n",
        "        test_text_str.append(transform_index_to_letter(cur_text, LETTER_LIST))\n",
        "\n",
        "    res_df = pd.DataFrame(test_text_str)\n",
        "    res_df.to_csv('result_{}.csv'.format(model_num + 1), index=True, header=False)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train start\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-dt9I8eZL8Z",
        "colab_type": "text"
      },
      "source": [
        "# 新段落"
      ]
    }
  ]
}